@inproceedings{10.1145/3746027.3754899,
author = {Li, Mingrui and Zhai, Shuhao and Zhao, Zibing and Sun, Luyue and Wang, Xinxiao and Li, Dong and Liu, Shuhong and Wang, Hongyu},
title = {Wild3A: Novel View Synthesis from Any Dynamic Images in Seconds},
year = {2025},
isbn = {9798400720352},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3746027.3754899},
doi = {10.1145/3746027.3754899},
abstract = {The core issue in novel view synthesis lies in how to handle dynamic scenes that are common in the real world, such as dynamic objects, occlusions, and varying luminance. Current 3D Gaussian Splatting-based methods perform excellently in static scenes but often rely on fixed camera parameters, precise semantic prior segmentation, or specially designed rendering loss functions when dealing with dynamic scenes. These additional pieces of information limit the method's generalization ability, real-time performance, and application in AR, VR, and multimedia. To address these issues, we propose Wild3A, a comprehensive end-to-end integrated framework: it directly regresses 3D point positions and initial point confidence via the MASt3R's Transformer and integrates a Bayesian estimation module based on multimodal information fusion. We adopt a self-supervised joint optimization approach for scene representation and camera parameters. Extensive experiments on both public and private datasets show that Wild3A effectively eliminates visual artifacts in various dynamic scenes, achieves state-of-the-art results across multiple tasks, and achieves real-time rendering at 1000+ FPS.},
booktitle = {Proceedings of the 33rd ACM International Conference on Multimedia},
pages = {7472â€“7480},
numpages = {9},
keywords = {3D reconstruction, dynamic suppression, multi-view geometry, novel view synthesis},
location = {Dublin, Ireland},
series = {MM '25}
}
